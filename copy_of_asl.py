# -*- coding: utf-8 -*-
"""Copy of ASL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12rjEUdxENUi2nNfJtWp13PTBraiRCGKE

# ASL to Text

## Setup
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
!pip install -q mediapipe==0.10.0
!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task

from google.colab import drive
drive.mount('/content/drive')

import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from mediapipe import solutions
from mediapipe.framework.formats import landmark_pb2
import numpy as np
import cv2
from google.colab.patches import cv2_imshow
import os

"""```python
!pip install kaggle
!mkdir /root/.kaggle/
!touch /root/.kaggle/kaggle.json
!echo '{"username":"tensorflowcolab","key":"883e5dc4a7759495e6fb43d9d5138528"}' > /root/.kaggle/kaggle.json
!chmod 600 /root/.kaggle/kaggle.json
!kaggle -h
!kaggle datasets download -d grassknoted/asl-alphabet
```

```python
from zipfile import ZipFile
with ZipFile('/content/drive/MyDrive/Colab Notebooks/ASL/data/asl-alphabet.zip', 'r') as zipObj:
   # Extract all the contents of zip file in current directory
   zipObj.extractall()
```

# MediaPipe Processing
1. Extract landmark locations and make matrix
2. Create skeleton images of the picture to train on

##1. Location Extraction
"""

import os

def make_df(directory):
  base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')
  options = vision.HandLandmarkerOptions(base_options=base_options,
                                       num_hands=2)
  detector = vision.HandLandmarker.create_from_options(options)
  out = []
  # count = 0
  for subdir, dirs, files in os.walk(directory):
    if files:
      for i in files:
        # count +=1
        # if count > 5:
          # break
        img = []
        path = os.path.join(subdir, i)
        image = mp.Image.create_from_file(path)
        detection_result = detector.detect(image).hand_landmarks
        if len(detection_result):
          for j in detection_result[0]:
            img.append([j.x, j.y, j.z, subdir[-1]])
          out.append(img)
  return np.asarray(out)

out = make_df("/content/drive/MyDrive/ASL/Test_Alphabet/")

"""##2. Skeleton Images"""

MARGIN = 10  # pixels

def draw_landmarks_on_image(rgb_image, detection_result):
  hand_landmarks_list = detection_result.hand_landmarks
  handedness_list = detection_result.handedness
  annotated_image = np.zeros(rgb_image.shape)

  # Loop through the detected hands to visualize.
  for idx in range(len(hand_landmarks_list)):
    hand_landmarks = hand_landmarks_list[idx]
    handedness = handedness_list[idx]

    # Draw the hand landmarks.
    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
    hand_landmarks_proto.landmark.extend([
      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks
    ])
    solutions.drawing_utils.draw_landmarks(
      annotated_image,
      hand_landmarks_proto,
      solutions.hands.HAND_CONNECTIONS,
      solutions.drawing_styles.get_default_hand_landmarks_style(),
      solutions.drawing_styles.get_default_hand_connections_style())

    # Get the top left corner of the detected hand's bounding box.
    height, width, _ = annotated_image.shape
    x_coordinates = [landmark.x for landmark in hand_landmarks]
    y_coordinates = [landmark.y for landmark in hand_landmarks]
    text_x = int(min(x_coordinates) * width)
    text_y = int(min(y_coordinates) * height) - MARGIN
  return annotated_image

classes = [chr(i) for i in range(65, 91)]
classes.append("Blank")

def pathfinder(directory):
  paths = []
  for subdir, dirs, files in os.walk(directory):
    if files:
      for i in files:
        path = os.path.join(subdir, i)
        paths.append(path)
  return paths
print(pathfinder("/content/drive/MyDrive/ASL/Test_Alphabet/")[:5])

def string_creater(l, start=True):
  string = "[" if start else ""
  for i in l:
    string += "["
    for j in i:
      string += "["
      for k in j:
        string += f"{k}, "
      string = string[:-2]
      string += "], "
    string = string[:-2]
    string += "], "
  return string
string_creater([[[1, 2, 3], [4, 5, 6]],[[7, 8, 9], [10, 11, 12]]])

def create_skeleton(directory, classes):
  out = []
  labels = []
  count = 0
  base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')
  options = vision.HandLandmarkerOptions(base_options=base_options,
                                              num_hands=2)
  detector = vision.HandLandmarker.create_from_options(options)
  i = open("/content/drive/MyDrive/ASL/progress1.txt", "r").read()[0]
  file1 = open("/content/drive/MyDrive/ASL/progress1.txt", "w")
  file2 = open("/content/drive/MyDrive/ASL/progress1.txt", "r")
  n =  int(i) if i.isdigit() else 0
  count = 0
  for path in pathfinder(directory)[n:]:
    subdir = path.split("/")[-2]
    image = mp.Image.create_from_file(path)
    detection_result = detector.detect(image)
    annotated_image = cv2.resize(draw_landmarks_on_image(image.numpy_view(), detection_result), (512, 512)).tolist()
    out.append(annotated_image)
    annote = "[" + str(annotated_image) if not count else str(annotated_image)
    file1.write(f"{n+count}\n{file2.read()[2:]}{annote}")
    labels.append(classes.index(subdir[-1]))
    count += 1
    if count == 6:
      break
create_skeleton("/content/drive/MyDrive/ASL/Test_Alphabet/", classes)

"""# Data Preprocessing"""

img = tf.constant(img)
img.shape

def get_labels(image, label):
  return image, label

def create_df(directory):
  imgs, labs = create_skeleton(directory)
  data = tf.data.Dataset.from_tensor_slices((tf.constant(imgs),
                                              tf.constant(labs)))
  data = data.shuffle(buffer_size=len(imgs))
  data_batch = data.map(get_labels).batch(32)
  return data_batch

string = open("/content/drive/MyDrive/ASL/progress1.txt", "r").read()[2:] + "]]"
arr = np.asarray(eval(string))

